# Copyright (C) 2009 Anand Patil
# 
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
# 
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
# 
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.


from optparse import OptionParser

# Create option parser

req_doc = """

mbg-validate  Copyright (C) 2009 Anand Patil
This program comes with ABSOLUTELY NO WARRANTY.
This is free software, and you are welcome to redistribute it under certain conditions.
See <http://www.gnu.org/licenses/> for the terms of the license.


  module                The module from which maps are to be generated.
  database-file         The name of the database file produced by the mcmc.
  burn                  The number of initial MCMC iterations to discard. 
  pred-pts              A csv file containing the lon, lat, (time,) and 
                        associated covariate values where you want to predict.
                        
                        NOTE: time must be in units of decimal years. That
                        is OK.

"""

p = OptionParser('usage: %prog module database-file burn pred-pts [options]' + req_doc)
p.add_option('-t','--thin',help='How much to thin the MCMC trace. Defaults to 1, meaning no thinning. This is recommended unless it takes too long.',dest='thin',type='int')
p.add_option('-i','--iter',help='The total number of samples to use in generating the map. Defaults to 20000',dest='total',type='int')
p.add_option('-d','--ignore-npd',help='Whether to discard and continue past non-positive-definite iterations.',dest='continue_past_npd',type='int')

p.set_defaults(thin=50)
p.set_defaults(continue_past_npd=0)
p.set_defaults(total=50000)


(o, args) = p.parse_args()
if len(args) != 4:
    raise ValueError, 'You must supply exactly four positional arguments. You supplied %i.'%len(args)

o.module, o.hf_name, o.burn, o.input_name = args
o.burn = int(o.burn)

import matplotlib
matplotlib.use('PDF')
matplotlib.interactive(False)

from map_utils import *
from generic_mbg import *
import tables as tb
import numpy as np
import os, imp, sys, time

import pylab as pl
from pylab import csv2rec    


# Load up given module and load its relevant contents

mod_path, mod_name = os.path.split(o.module)
mod_basename, mod_ext = os.path.splitext(mod_name)
mod_search_path = [mod_path, os.getcwd()] + sys.path
mod = imp.load_module(mod_basename, *imp.find_module(mod_basename, mod_search_path))


# Parse input file
input = csv2rec(file(o.input_name,'U'))
mod.check_data(input)

from numpy.ma import mrecords
if isinstance(input, np.ma.mrecords.MaskedRecords):
    msg = 'Error, could not parse input at following rows:\n'
    for name in input.dtype.names:
        if np.sum(input[name].mask)>0:
            msg += '\t%s: %s\n'%(name, str(np.where(input[name].mask)[0]+1))
    raise ValueError, msg

lon = maybe_convert(input, 'lon', 'float')
lat = maybe_convert(input, 'lat', 'float')
if hasattr(input, 't'):
    t = maybe_convert(input, 't', 'float')
    x = combine_st_inputs(lon,lat,t)
else:
    x = combine_spatial_inputs(lon,lat)
    
non_cov_columns = {}
if hasattr(mod, 'non_cov_columns'):
    non_cov_coltypes = mod.non_cov_columns
else:
    non_cov_coltypes = {}
non_cov_colnames = non_cov_coltypes.keys()

covariate_dict = {}
for n in input.dtype.names:
    if n not in ['lon','lat','t']:
        if n in non_cov_colnames:
            non_cov_columns[n] = maybe_convert(input, n, non_cov_coltypes[n])
        else:
            covariate_dict[n]=maybe_convert(input, n, 'float')


# Initialize model using preexisting DB
M = create_model(mod, pm.database.hdf5.load(o.hf_name))
hf = M.db._h5file
nuggets = dict([(getattr(M,k),getattr(M,v)) for k,v in mod.nugget_labels.iteritems()])


# Sort out the observed values and the postprocessing functions
if np.iterable(mod.validate_postproc):
    postproc_fns = mod.validate_postproc
else:
    postproc_fns = [mod.validate_postproc]

vp = []
obs = []
n = []
for postproc_fn in postproc_fns:
    obs_, n_, vp_ = postproc_fn(input)
    obs.append(obs_)
    n.append(n_)
    vp_.__name__ = postproc_fn.__name__
    vp.append(vp_)

t1 = time.time()
products = hdf5_to_samps(M,x,nuggets,o.burn,o.thin,o.total,[sample_reduce], vp, covariate_dict, sample_finalize, continue_past_npd=o.continue_past_npd)
print '\nPredictive samples drawn in %f seconds\n'%(time.time() - t1)

# Write out
hf_path, hf_basename  = os.path.split(o.hf_name)
base_outname = os.path.splitext(hf_basename)[0]
val_dir = os.path.join(hf_path, base_outname+'-validation')
try:
    os.mkdir(val_dir)
except OSError:
    pass
os.chdir(val_dir)
samps_h5file = tb.openFile(base_outname+'-validation-samples.hdf5', 'w')

# Record observed values
samps_h5file.createTable('/','observed',np.rec.fromarrays(obs, names=','.join([f.__name__ for f in vp])))
samps_h5file.createTable('/','n',np.rec.fromarrays(obs, names=','.join([f.__name__ for f in vp])))
samps_h5file.createGroup('/','predicted')
for f, obs_, n_ in zip(vp, obs, n):
    
    # Record raw predicted samples
    samps = products[f]['samples'].T
    if samps.shape[0] != (len(n_)):
        raise ValueError, 'Samps is wrong shape'
    samps_h5file.createCArray('/predicted',f.__name__,tb.FloatAtom(),samps.shape,filters=tb.Filters(complevel=1,complib='zlib'))[:]=samps
    
    n_ = np.asarray(n_).astype('float')
    samps_h5file.root.n.col(f.__name__)[:]=n_
    
    errors = np.rec.fromarrays([np.empty(len(input)) for i in (0,1,2)], names='mean error,mean abs error,RMS error')

    for i in xrange(len(input)):
        
        obs_frequency = obs_[i] / n_[i]
        frequency_dev = samps[i] / n_[i] - obs_frequency

        errors['mean error'][i] = np.mean(frequency_dev)
        errors['mean abs error'][i] = np.mean(np.abs(frequency_dev))
        errors['RMS error'][i] = np.sqrt(np.mean(frequency_dev**2))
                
        pl.clf()
        pdf, bins, patches = pl.hist(samps[i], 50, normed=True,facecolor='.3')
        yext = pdf.max()
        pl.plot([obs_[i],obs_[i]],[0,yext],'r-.',linewidth=3,label='Observed %s'%f.__name__)
        pl.legend(loc=0)
        pl.xlabel(f.__name__)
        pl.ylabel('Predictive density')
        pl.title('(%f, %f)'%(lon[i],lat[i]))
        pl.savefig('%i.pdf'%i)
    
    pl.rec2csv(errors,'%s-mean-errors.csv'%f.__name__)

    for vf in validation_fns:
        print 'Generating plot of %s for %s'%(vf.__name__, f.__name__)
        pl.clf()
        res=vf((samps.T/n_), samps.T, obs_, n_-obs_)
        pl.savefig(f.__name__+'-'+vf.__name__ + '.pdf', transparent=True)
        pl.rec2csv(res,f.__name__+'-'+vf.__name__+'.csv')