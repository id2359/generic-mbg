\chapter{Statistician's guide} 
\label{chap:stat}

This chapter explains how to write specializing modules. A specializing module is a Python package that defines a PyMC probability model. It needs to expose certain attributes that tell \texttt{generic-mbg} how to interpret the variables the model contains.

This chapter assumes you know how to use PyMC. See the \href{http://code.google.com/p/pymc/}{PyMC user guide} if you don't.




\section{File structure} 
Try the following from the shell, on a single line:
\begin{verbatim}
    mbg-init-specializing-module duffy-vivax 
        -e 'cut_matern, utils' 
        -a 'Anand Patil, Carlos Guerra, Rosalind Howes'
\end{verbatim}
The command \texttt{mbg-init-specializing-module} is there to help you get the file structure of your specializing module right. It will create Fortran extensions according to the \texttt{-e} option, add the commands to compile them in \texttt{setup.py}, give the package's copyright to the given authors and license it under the GPL. It will also initialize a git repository in the new directory, and make the initial commit.


Take a moment to browse the directory tree you've created, which should look like this:
\begin{center}
\includegraphics[width=12cm]{filestructure.png}     
\end{center}
Pay special attention to the \texttt{\_\_init\_\_.py} file, which lists the attributes you'll need to define to tell \texttt{generic-mbg} how to use your specializing module.

\subsection{Importing}
You can install the new package as follows:
\begin{verbatim}
    cd duffy-vivax
    python setup.py develop
\end{verbatim}
See section \ref{sec:spec-local} for more detail. Now change directory to \texttt{\~}, start the Python interpreter, and import the \texttt{duffy\_vivax} module. Browse through its namespace. Pay special attention to the calling conventions of the Fortran subroutines you've generated:
\begin{verbatim}
    In [2]: duffy_vivax.utils.dummy?
    Type:           fortran
    String Form:    <fortran object at 0x1067c4828>
    Namespace:      Interactive
    Docstring:
        dummy - Function signature:
          y = dummy(x)
        Required arguments:
          x : input rank-1 array('d') with bounds (n)
        Return objects:
          y : rank-1 array('d') with bounds (n)
\end{verbatim}
The \texttt{f2py} directives in \texttt{utils.f} (the lines beginning with \texttt{cf2py}) determine which arguments to the subroutine are inputs you need to provide from Python, which inputs Python should see as outputs, and which should be inferred from other arguments. Learning how to write your own Fortran extensions is very worthwhile.

\section{Writing the model}

The primary attribute of a specializing module is a function called \texttt{make\_model}, henceforth the `model factory'. The model factory should take the following arguments:
\begin{description}
    \item[lon] An array of longitudes, in radians.
    \item[lat] An array of latitudes, in radians, of the same length as \texttt{lon}.
    \item[t (optional)] An array of times, in decimal years, of the same length as \texttt{lon}. Spatiotemporal models only.
    \item[covariate\_dict] A dictionary mapping labels to arrays. Each array should give the evaluation of a covariate surface on the \texttt{[lon,lat,t]} input array defined so far.
    \item[**non\_cov\_columns] Any number keyword arguments whose values are arrays of the same length as \texttt{lon}.
\end{description}
These arguments will be read from a user-supplied datafile and fed into the model factory by \texttt{mbg-infer}. The model factory should return its local variables as follows: 
\begin{verbatim}
    def make_model(lon,lat,covariate_dict,**non_cov_columns):
        ...
        return locals()
\end{verbatim}

\subsection{The fields}

\texttt{Generic-mbg} handles probability models containing one or more Gaussian random fields, whose means are linear combinations of covariate surface evaluations with normally-distributed coefficients, with normally-distributed nuggets:
\begin{equation}
    \label{eq:canonical} 
    \begin{array}{r}
        \beta \stackrel{\textup{\tiny iid}}{\sim} \textup{N}(0,V_{\beta,i}) \\\\
        M:x\rightarrow f_0(x;\phi) + \beta_0 + \sum_{i=1}^n \beta_i c_i(x)\\\\
        C:x,y\rightarrow K(x,y;\theta)\\\\
        f \sim \textup{GP}(M,C)\\\\
        f_x = f(x) \sim \textup{N}(M(x), C(x,x))\\\\
        g_x \sim \textup{N}(f_x, V) 
    \end{array}
\end{equation}
where $K$ is a positive definite covariance function with parameters $\theta$ and $f_0$ is any old function with parameters $\phi$. \texttt{Generic-mbg} does not care what priors you use for $\theta$, $\phi$, $V$ or the $V_\beta$'s, but the coefficients $\beta$ must be iid and normally distributed. Similarly, the package doesn't care how any other variables depend on any of the variables in the model. There may be any number of these Gaussian process submodels in a model.

The nugget and covariates are optional. Without both, the model would be simplified:
\begin{eqnarray*}
    \beta_0\sim\textup{N}(0,V_{\beta,0})\\
    M:x\rightarrow f_0(x;\phi) + \beta_0\\
    C:x,y\rightarrow K(x,y;\theta)\\
    f \sim \textup{GP}(M,C)\\
    f_x = f(x) \sim \textup{N}(M(x), C(x,x))
\end{eqnarray*}

\subsection{Integrating out the covariates}

Using standard multivariate normal transformation rules, the first four lines of model (\ref{eq:canonical}):
\begin{eqnarray*}
    \beta \stackrel{\textup{\tiny iid}}{\sim} \textup{N}(0,V_{\beta,i}) \\
    M:x\rightarrow f_0(x;\phi)+\beta_0 + \sum_{i=1}^n \beta_i c_i(x)\\
    C:x,y\rightarrow K(x,y;\theta)\\
    f \sim \textup{GP}(M,C)
\end{eqnarray*}
can be marginalized to obtain the following model:
\begin{equation}
    \label{eq:int-covariates} 
    \begin{array}{r}
    M:x\rightarrow f_0(x;\phi)\\\\
    C:x,y\rightarrow K(x,y;\theta) + V_{\beta_0} + \sum_{i=1}^n V_{\beta,i}c_i(x)c_i(y)^T\\\\
    f\sim\textup{GP}(M,C)
    \end{array}
\end{equation}
This parameterization is required in \texttt{generic-mbg}, because experience shows that the MCMC chains tend to mix much better. Section \ref{sub:example} shows how to do this easily. The shell command \texttt{mbg-covariate-traces} can be used to obtain MCMC traces for the $\beta$'s after the MCMC is complete.

\subsection{The nugget and mixing}

The last two lines of model \ref{eq:canonical},
\begin{eqnarray*}
    f_x = f(x) \sim \textup{N}(M(x), C(x,x))\\
    g_x \sim \textup{N}(f_x, V)
\end{eqnarray*}
are slightly nonstandard. Usually, the nugget variance $V$ would be incorporated in the covariance function, and $f_x$ would not be directly imputed. Keeping $f_x$ and $g_x$ separate is strongly recommended in \texttt{generic-mbg}, however. Experience has shown that the following jumping strategy:
\begin{enumerate}
    \item Metropolis sample each element of $g_x$ one at a time
    \item Gibbs sample $f_x$ jointly conditional on $g_x$
\end{enumerate}
performs well in a wide range of situations. The second step can be performed using the step method \texttt{GPEvaluationGibbs}, which is provided by \texttt{PyMC}. Section \ref{sub:mcmc-init} describes how to use \texttt{GPEvaluationGibbs}. 

\subsection{Example field}
\label{sub:example} 

Let's make the submodel in equation \ref{eq:canonical}, reparameterized as in equation \ref{eq:int-covariates}. First, the covariance function and its parameters:
\begin{verbatim}
    amp = pymc.Exponential('amp', .1, value=1.)
    scale = pymc.Exponential('scale_shift', .1, value=.08)
    diff_degree = pymc.Uniform('diff_degree', .01, 1.5)
    covariate_variances = pymc.Uniform('covariate_variances',0,1,size=n_covariates)
    x = numpy.hstack((lon,lat)).T
    
    @pymc.deterministic
    def C(amp=amp, scale=scale, diff_degree=diff_degree, 
            covariate_variances=covariate_variances):

        facdict = dict([(k,1e2*covariate_variances[i]\
            for i,k in enumerate(covariate_names))])
            
        facdict['m']=1e4

        eval_fun = generic_mbg.CovarianceWithCovariates(pm.gp.cov_funs.matern.geo_rad,
            x, covariate_values, fac=facdict)

        return pymc.gp.FullRankCovariance(eval_fun, amp=amp, scale=scale, 
            diff_degree=diff_degree)
\end{verbatim}
\texttt{Generic-mbg} provides the wrapper class \texttt{CovarianceWithCovariates}, which creates a covariance function of the form shown in (\ref{eq:int-covariates}). The key \texttt{'m'} is understood to correspond to the constant term. As formulated, the variances of the covariate coefficients are free parameters in the model, meaning the model can simplify itself. Now a trivial mean function:
\begin{verbatim}
    M = pymc.gp.Mean(lamda x: numpy.zeros(x.shape[0]))
\end{verbatim}

The PyMC factory class \texttt{GPSubmodel} incorporates a Gaussian process with the given mean and covariance into the model:
\begin{verbatim}
    S = pymc.gp.GPSubmodel('S', M, C, x)
\end{verbatim}

Now we're ready to create $g_x$:
\begin{verbatim}
    g_x = pymc.Normal('g_x', mu=S.f_eval, tau=1/V)
\end{verbatim}
PyMC parameterizes the normal distribution by precision rather than variance, so the second parameter of $g_x$ is $\tau=1/V$. The mean is the \texttt{f\_eval} attribute of the GP submodel, which gives the GP's evaluation on the input mesh, in this case \texttt{x}. That's all there is to producing GP submodels in \texttt{generic-mbg}. You can complete the model any way you like: add more GP submodels, likelihoods, etc.

\subsection{Be sure to commit}
\label{sub:git-commit} 
It's important to commit any changes you make to the specializing module to the local repository before running any MCMC, because the hash of the current git head is stored in the MCMC trace. If the state of the code doesn't correspond to the hash, you may not be able to reproduce your results later.

\section{Required attributes}

The specializing module needs to expose several attributes in addition to \texttt{make\_model}. See \texttt{\_\_init\_\_.py} in the package you just created, \texttt{duffy\_vivax}

\subsection{Metadata and non-covariate columns}
\subsubsection{Non-covariate columns} 
The attribute \texttt{non\_cov\_columns} is a dictionary mapping labels to type strings (such as \texttt{'float'}, \texttt{'int'}, \texttt{'str'}, etc.). \texttt{Generic-mbg} will expect to find the labels as column headers in any CSV datafiles provided by the end user. 
\subsubsection{Arbitrary metadata} 
The attribute \texttt{metadata\_keys} allows you to store any local variable generated by the model factory in the trace. If your \texttt{metadata\_keys} are \texttt{['n\_tacos', 'fillings']}, then the corresponding variables can be retrieved from the trace file using \texttt{hf.root.metadata.n\_tacos[0]}, etc.

\subsection{Tagging the nuggets of the Gaussian process submodels}
\label{sub:variable-tags} 
The following variables must be defined in a specializing module to tag certain variables created by the model factory as playing certain roles in GP submodels:
\begin{description}
    \item[\texttt{nugget\_labels}] A dictionary mapping the names of the GP submodels to the labels of their associated nugget variances, ie \texttt{\{'S': 'V'\}}.
    \item[\texttt{obs\_labels}] A dictionary mapping the names of the GP submodels to the labels of their associated evaluations with nugget, ie \texttt{\{'S': 'g\_x'\}}.
\end{description}


\subsection{Postprocessing for map generation}
The specializing module must expose a list of functions called \texttt{map\_postproc}. Each of these functions should take arguments corresponding to all the Gaussian process submodels in the model, and return... whatever it is you want to show the per-pixel predictive distribution of in the maps. For example, if your model is a spatial GLM with count data and an inverse-logit link function, and the name of the lone GP submodel in the model is $S$, this function would do:
\begin{verbatim}
    def prevalence(S):
        return generic_mbg.invlogit(S)
    map_postproc = [prevalence]
\end{verbatim}
\texttt{Generic-mbg}'s link functions should be preferred to all others when possible, because they're multithreaded.

The map postprocessing function can take additional arguments corresponding to tallied variables in the probability model. For example, if you wanted to use Stukel's link function, and had shape variables $a_1$ and $a_2$ in the model:
\begin{verbatim}
    def prevalence(S, a1, a2):
        return generic_mbg.stukel_invlogit(S, a1, a2)
    map_postproc = [prevalence]
\end{verbatim}
Note that the GP submodel arguments are understood to be per-pixel realizations on the prediction grid with nugget variance incorporated.

\subsection{Postprocessing for validation} 
The \texttt{generic-mbg} validation scheme is to choose one or more summaries of the data that can be considered the results of random trials, and to compare the model's prediction of the outcomes of these trials given a subset of the data to the observed outcomes in the rest of the data. The specializing module must expose a list of functions called \texttt{validate\_postproc} that define these summaries. 

The validation trials may depend on some aspects of the held-out dataset, for example the ages of the survey participants. For that reason, each of the functions in \texttt{validate\_postproc} must take a single argument, which is the held-out dataset represented as a record array. Each should return three values: the `number successful', the `number of attempts', and a function. The function should have the same calling conventions as those in \texttt{map\_postproc}: it should take the GP submodels, and optionally some variables in the model, as arguments. It should return a predictive sample for the `number successful' given the values of the model variables and possibly the `number of attempts'. For example:
\begin{verbatim}
    def prevalence(data):
        obs = data.pos
        n = data.pos + data.neg
        def f(S, a1, a2, n=n):
            return pm.rbinomial(n, generic_mbg.stukel_invlogit(S, a1, a2))
        return obs, n, f

    validate_postproc=[pr]    
\end{verbatim}

The shell command \texttt{mbg-validate} will read the non-covariate columns from the held-out datasheet, pass them into \texttt{validate\_postproc}, and use the returned function to convert the Gaussian field realizations to predictions. As with \texttt{map\_postproc}, the GP submodel arguments received will be per-pixel realizations on the held-out locations with nugget variance incorporated.

\subsection{Postprocessing for areal prediction}  

The task performed by the shell command \texttt{mbg-areal-predict} has a more complicated specification than the tasks performed by the other shell commands. The problem tackled by \texttt{mbg-areal-predict} is samples from, and summaries of, the posterior predictive distribution of quantities of the form
\begin{equation}
    \label{eq:areal-targ} 
    h\left(\frac{1}{|a_1|_w}\int_{a_1\times \tau_1} g_1(f_1,\ldots,f_n,x,\tau)w(x)da, \ldots,\frac{1}{|a_2|_w}\int_{a_m\times\tau_m} g_m(f_1,\ldots,f_n,x,\tau)w(x)da\right)
\end{equation}
where $a_1,\ldots,a_m$ are regions and $da$ denotes integration over area; $w$ is a weighting function and $|a|_w=\int_a w(x)da$; $\tau_1\ldots\tau_m$ are time intervals; $x$ is a geographical location; $f_1,\ldots,f_n$ are the Gaussian random fields in the model; and $g_1\ldots g_m$ and $h$ are arbitrary functions. This general specification covers many `volumetric' estimates of epidemiological interest. Three concrete examples are given below.

\begin{description}
    \item[\emph{Plasmodium falciparum} clinical burden in a country.] In this case, $n=m=1$. $f_1$ is the Gaussian random field, with nugget incorporated, corresponding to parasite rate. $g_1$ transforms this value to parasite rate (with a random age-correction factor), and thence to clinical incidence, multiplies it by the population density of the pixel containing $x$, and integrates over $\tau$, being a particular year. $h$ multiplies its argument by $|a|^2$.
    \item[The total change in \emph{Plasmodium falciparum} clinical burden in a country between two consecutive years.] In this case $m=2$ with $a_1=a_2$ and $\tau_1=[y,y+1]$ and $\tau_2=[y+1,y+2]$, $y$ being the first of the two years. $f$ and $g$ are as in the previous example. $h(z_1,z_2)=(z_2-z_1)|a|$.
    \item[The difference in areal mean HbS prevalence between Lysenko endemicity classes.] In this case $m=5$ (the number of Lysenko classes) and $n=1$. $a_i$ is Lysenko class $i$ and $\tau$ is not needed, since the model is atemporal. $f_1$ is the Gaussian random field corresponding to HbS allele frequency. $g_i$ applies the inverse link function to its argument. $h$ maps vectors of length $n$ to vectors of length $n-1$: $h(z_1,\ldots,z_n)=[z_2-z_1,\ldots,z_n-z_{n-1}]$.
    
The regions $a_1,\ldots,a_m$ and time intervals $\tau_1,\ldots,\tau_m$ are defined in the argument \texttt{polyfile} of \texttt{mbg-areal-predict}. The regions are given as \href{http://geojson.org/geojson-spec.html}{GeoJSON} polygons or multipolygons, contained in geometry collections. The associated time intervals are defined by properties \texttt{tmin} and \texttt{tmax}. An example geometry collection called \texttt{gc1} containing two regions ($r_1$ and $r_2$) is given below.
\end{description}
\begin{verbatim}
    {"type": "GeometryCollection",
    "geometries":[
        {"type": "GeometryCollection",
        "geometries": [
            {"type": "MultiPolygon", 
                "coordinates": [[[[0.64, 5.85], [0.66, 5.75], [0.26, 5.76], [-0.8, 5.21], 
                            [-1.61, 5.02], [-2.06, 4.73], [-2.93, 5.1], [-3.25, 6.61], 
                            [-2.49, 8.2], [-2.69, 9.48], [-2.83, 11], [-0.15, 11.14], 
                            [0.73, 8.32], [0.53, 6.95], [1.2, 6.1], [0.69, 5.75], 
                            [0.63, 5.95], [0.51, 6.06], [0.41, 6.08], [0.37, 6.04], 
                            [0.26, 6.1], [0.21, 6.09], [0.36, 6.02], [0.42, 6.07], 
                            [0.49, 6.04], [0.61, 5.95], [0.64, 5.85]]]], 
                "properties": {"name": "late", "tmin": 2000.0, "tmax": 2007.5}
            },

            {"type": "MultiPolygon", 
                "coordinates": [[[[0.64, 5.85], [0.66, 5.75], [0.26, 5.76], [-0.8, 5.21], 
                            [-1.61, 5.02], [-2.06, 4.73], [-2.93, 5.1], [-3.25, 6.61], 
                            [-2.49, 8.2], [-2.69, 9.48], [-2.83, 11], [-0.15, 11.14], 
                            [0.73, 8.32], [0.53, 6.95], [1.2, 6.1], [0.69, 5.75], 
                            [0.63, 5.95], [0.51, 6.06], [0.41, 6.08], [0.37, 6.04], 
                            [0.26, 6.1], [0.21, 6.09], [0.36, 6.02], [0.42, 6.07], 
                            [0.49, 6.04], [0.61, 5.95], [0.64, 5.85]]]], 
                "properties": {"name": "early", "tmin": 1993.5, "tmax": 2000.0}
            }
        ],
        "properties" : {"name": "Ghana"}
        },
        {"type": "GeometryCollection",
            "geometries": [
                {"type": "MultiPolygon", 
                    "coordinates": [[[[1.4, 9.43], [1.64, 6.22], [1.2, 6.1], [0.53, 6.95], 
                                [0.73, 8.32], [-0.15, 11.14], [0.92, 11], [0.78,  10.38], 
                                [1.4, 9.43]]]], 
                    "properties": {"name": "late", "tmin": 2000.0, "tmax": 2007.5}
                },

                {"type": "MultiPolygon", 
                    "coordinates": [[[[1.4, 9.43], [1.64, 6.22], [1.2, 6.1], [0.53, 6.95], 
                                [0.73, 8.32], [-0.15, 11.14], [0.92, 11], [0.78,  10.38], 
                                [1.4, 9.43]]]], 
                    "properties": {"name": "early", "tmin": 1993.5, "tmax": 2000.0}
                }
            ],
            "properties" : {"name": "Togo"}
            }
        ]
    }
\end{verbatim}

The file may contain multiple geometry collections, each of which is independently subjected to an estimation procedure of the form given above. 

The specializing module must expose an attribute called \texttt{areal\_postproc}, which like \texttt{map\_postproc} and \texttt{validate\_postproc} must be a list of functions. Each must take a Shapely geometry collection as its lone argument and return two values: $h$, which must be of appropriate arity to fulfil its role given above; and $g$, which must be a dictionary of functions keyed by area name. Each value in $g$ mist take as arguments the random fields in the model, an array of locations (lat/lon) an array of times for temporal models, and optionally any other variables in the model. The following specifications might implement concrete examples above:
\begin{verbatim}
    def prevalence(gc):
        "Plasmodium falciparum clinical burden in a country"
        def h(z,a=gc.geoms[0].area):
            return z*a**2
        def g(S,x,t,a1,a2):
            return generic_mbg.stukel_invlogit(S(np.hstack((x,t))), a1, a2)
        return h, {gc.geoms[0].name: g}
    
    def diff_prevalence(gc):
        """Total change in Plasmodium falciparum clinical burden in a country 
        between two consecutive years"""
        def h(z1,z2,a=gc.geoms[0].area):
            return (z2-z1)*a
        def g(S,x,t,a1,a2):
            return generic_mbg.stukel_invlogit(S(np.hstack((x,t))), a1, a2)
        return h, dict([geom.name, g] for geom in gc.geoms)
    
    def hbs_diff(gc):
        "Difference in areal mean HbS prevalence between Lysenko endemicity classes"
        def h(*l):
            return np.diff(l)
        g = dict([geom.name, lambda S, x, a=geom.area: generic_mbg.invlogit(S(x))/a]\
            for geom in gc.geoms)
        return h, g
\end{verbatim}

\subsubsection{Temporal alignment}
By default, the $g$ function is interpreted as $\tilde g$ in $g(f_1,\ldots,f_n,x,\tau)=\frac{1}{|\tau|}\int_\tau \tilde g(f_1(x,t),\ldots,f_n(x,t))dt$. In this default case, the $x$ and $t$ arguments seen by $g$ are such that \texttt{np.hstack((x,t))} gives an array of space-time samples distributed within $a\times\tau$ either uniformly or according to the weighting raster. The two temporal examples given above fall into this case.

However, consider the case $g(f_1,\ldots,f_n,x,\tau)=k(\int_\tau\tilde g(f_1(x,t),\ldots,f_n(x,t))dt)$, where $k$ is a nonlinear transformation. The estimate \ref{eq:areal-targ} becomes difficult to compute based on uniformly-distributed $x,t$ points in $a\times \tau$. In this case, the function signature of $g$ should be \texttt{g(f1,...,fn,x,tau,...)} and the \texttt{-c} option to \texttt{mbg-areal-predict} should be set. $g$ should generate vectors of time points at each location in $x$ as necessary. If the same $f$'s are used for all time vectors, they will all be jointly simulated.

\subsubsection{Estimation and Monte Carlo standard error}
\texttt{Mbg-areal-predict} is unique amongst the \texttt{generic-mbg} commands in that its Monte Carlo error will generally be of more concern. For this reason, its \texttt{-r} argument determines the number of replications. All computations are repeated this many times. The averages of the results are presented as estimates, and the standard deviations are scaled and presented as estimates of Monte Carlo standard error.

\textbf{Document output format once you produce a data structure}

\subsection{Initializing the MCMC object}
\label{sub:mcmc-init} 

The shell command \texttt{mbg-infer} will call \texttt{make\_model} and wrap the jumble of variables it returns in a PyMC \texttt{MCMC} object. Before it begins sampling, it will call \texttt{mcmc\_init} on the MCMC object. This is your chance to assign custom step methods, etc. For example, to use \texttt{PyMC}'s \texttt{GPEvaluationGibbs} on the Gaussian field evaluation in section \ref{sub:example}:
\begin{verbatim}
    def mcmc_init(M):
        M.use_step_method(pymc.GPEvaluationGibbs, M.S, M.V, M.g_x)
\end{verbatim}



