# Copyright (C) 2010 Anand Patil
# 
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
# 
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
# 
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from optparse import OptionParser
import os, sys, time, imp, datetime

# Create option parser
req_doc = """

mbg-areal-predict  Copyright (C) 2010 Anand Patil
This program comes with ABSOLUTELY NO WARRANTY.
This is free software, and you are welcome to redistribute it under certain conditions.
See <http://www.gnu.org/licenses/> for the terms of the license.

See the PDF documentation of generic-mbg.

  module                The module from which maps are to be generated.
  database-file         The name of the database file produced by the mcmc.
  burn                  The number of initial MCMC iterations to discard. 
                        You must provide this argument.
  polyfile              A file containing the regions to consider in GeoJSON
                        format. See the PDF documentation.
"""
p = OptionParser('usage: %prog module database-file burn mask [options]' + req_doc)
p.add_option('-r','--reps',help='The number of repetitions to do, for purposes of computing Monte Carlo standard error. Defaults to 10.',dest='reps',type='int')
p.add_option('-n','--n-bins',help='The number of bins to use when creating histograms. Defaults to 100.',dest='n_bins',type='int')
p.add_option('-x','--points',help='The number of spatial points to use for estimating areal integrals. Defaults to 100.',dest='points',type='int')
p.add_option('-i','--iter',help="The number of posterior predictive draws to take for each repetition. defaults to 1000.",dest='total',type='int')
p.add_option('-t','--thin',help='How much to thin the MCMC trace. Defaults to 10.',dest='thin',type='int')
p.add_option('-w','--weight-raster',help="An optional raster giving relative weights. This can be used if weighted samples will give a better approximation to the integral than multiplying the integrand by the weight.",dest='weight_raster')
p.add_option('-c','--coordinate-time',help="If 0, (x,t) samples are distributed randomly in the admin unit X the time interval. If 1, then to each 'x' sample there correspond multiple 't' samples and vice versa. Set this to true if your f's need to take an integral in time (or space), apply a nonlinear transformation, and then take an integral in space (or time).",dest='coordinate_time')
p.add_option('-m','--time-points',help="If the coordinate-time flag is set, this option determines how many time points are stacked up. Defaults to 10.",dest='time_points')
p.add_option('-d','--ignore-npd',help='Whether to discard and continue past non-positive-definite iterations.',dest='continue_past_npd',type='int')
p.add_option('-q','--quantiles',help="The quantile maps to generate. Should be in the form '0.05 0.25 0.5 0.75 0.95', and the inverted commas are important! Defaults to '0.05 0.25 0.5 0.75 0.95'",dest='quantile_list')
p.add_option('-p','--raster-path',help="The path to the covariate rasters. Defaults to the current working directory.",dest='raster_path')
p.add_option('-u','--quantile-uplim',help='The upper limit of the mapped quantiles',type='float',dest='quantile_uplim')
p.add_option('-l','--quantile-lolim',help='The lower limit of the mapped quantiles',type='float',dest='quantile_lolim')


p.set_defaults(reps=10)
p.set_defaults(points=100)
p.set_defaults(total=1000)
p.set_defaults(weight_raster=None)
p.set_defaults(coordinate_time=0)
p.set_defaults(continue_past_npd=0)
p.set_defaults(thin=10)
p.set_defaults(quantile_list='0.05 0.25 0.5 0.75 0.95')
p.set_defaults(raster_path='.')
p.set_defaults(quantile_uplim=1.)
p.set_defaults(quantile_lolim=0.)
p.set_defaults(n_bins=100)
p.set_defaults(time_points=10)



o, args = parse_and_check_args(p,4)

if o.coordinate_time:
    raise NotImplementedError, "Haven't implemented time coordination yet."
        
o.module, o.hf_name, o.burn, o.polyfile_name = args
o.burn = int(o.burn)

from generic_mbg import *
exec(delayed_import_str)

M, hf, mod, mod_name = reload_model(o.module, o.hf_name)
nuggets, obs_labels = get_nuggets_and_obs(mod, mod_name, M)
extra_reduce_fns, extra_finalize = get_reduce_finalize(mod)

# Parse quantiles
if len(o.quantile_list) == 0:
    q = []
else:
    q = map(float, o.quantile_list.split(' '))

# Restore model
temporal = ('t' in hf.root.input_csv.colnames)
bins = np.linspace(0,1,o.n_bins)

# Create covariate locations
ck = copy.copy(M.covariate_keys)
cov_bbox = None
try:
    ck.remove('m')
except ValueError:
    pass
if len(ck)>0:
    x, unmasked, output_type = raster_to_locs(ck[0], thin=1, bufsize=0, path=o.raster_path)
    cov_lon, cov_lat, cov_data, cov_type = import_raster(ck[0], o.raster_path)
    cov_mask = cov_data.mask
    cov_bbox = x[:,0].min(), x[:,1].min(), x[:,0].max(), x[:,1].max()
    all_covariate_keys = M.covariate_keys
    all_covariate_keys.remove('m')
else:
    all_covariate_keys = []

if o.weight_raster is not None:
    if cov_bbox is None:
        x, unmasked, output_type = raster_to_locs(o.weight_raster, thin=1, bufsize=0, path=o.raster_path)
        cov_lon, cov_lat, cov_data, cov_type = import_raster(o.weight_raster, o.raster_path)
        cov_mask = cov_data.mask
        cov_bbox = x[:,0].min(), x[:,1].min(), x[:,0].max(), x[:,1].max()        
    weights = raster_to_vals(o.weight_raster, path=o.raster_path, thin=1, unmasked=unmasked)
else:
    weights, cov_lon, cov_lat = None, None, None

# Create utility fns
if len(q)>0:
    def binfn(arr,n_bins=o.n_bins,qu=o.quantile_uplim,ql=o.quantile_lolim):
        return np.array(((arr-ql)/(qu-ql))*n_bins,dtype=int)

    hsr = histogram_reduce(bins, binfn)
    hsf = histogram_finalize(bins, q, hsr)

def finalize(prod, n, q=q, ef=extra_finalize, qu=o.quantile_uplim, ql=o.quantile_lolim):
    mean = prod[mean_reduce] / n
    var = prod[var_reduce] / n - mean**2
    std = np.sqrt(var)
    std_to_mean = std/mean
    out = {'mean': mean, 'var': var, 'std': std, 'std-to-mean':std_to_mean}
    if len(q)>0:
        fin = hsf(prod, n)
        out.update(dict([(k,fin[k]*(qu-ql)+ql) for k in fin.iterkeys()]))
    if ef is not None:
        out.update(ef(prod, n))
    return out

reduce_fns = [mean_reduce, var_reduce]
if len(q)>0:
    reduce_fns = reduce_fns + [hsr]

# Slurp the polyfile
import geojson
from shapely.geometry import asShape
gjdat = geojson.loads(file(o.polyfile_name).read())
mps = {}
for geomcoll in gjdat['geometries']:
    mps_ = {}
    for geom in geomcoll['geometries']:
        g = asShape(geom)
        if cov_bbox is not None:
            if not box_inside_box(np.array(g.envelope.bounds)*np.pi/180., cov_bbox):
                raise ValueError, 'Multipolygon "%s" in geometry collection "%s" is not inside the covariate raster.'%(geom['properties']['name'],geomcoll['properties']['name'])
            # Report the number of missing pixels inside the unit.
            mask_in_unit = rastervals_in_unit(g, cov_lon.min(), cov_lat.min(), cov_lon[1]-cov_lon[0], cov_mask, view='y-x+')
            frac_masked = np.sum(mask_in_unit)/float(len(mask_in_unit))
            if frac_masked>0:
                warnings.warn('%f of the pixels in multipolygon "%s" in geometry collection "%s" are missing.'%(frac_masked,geom['properties']['name'],geomcoll['properties']['name']))
        if temporal:
            mps_[geom['properties']['name']]={'geom':g,'tmin':geom['properties']['tmin'],'tmax':geom['properties']['tmax']}
        else:
            mps_[geom['properties']['name']]={'geom':g}
    mps[geomcoll['properties']['name']]=mps_


# For each geometry collection:
t_start = time.time()
print '\nAreal predictions produced in %f seconds\n'%(time.time() - t_start)
points = {}
# def hdf5_to_areal_samps(M, x, nuggets, burn, thin, total, fns, h, g, pred_covariate_dict, finalize=None, continue_past_npd=False, joint=False):
"""
Parameters:
    M : MCMC object
        Its database should have samples in it.
    x : dict of dicts of arrays
        {geomname: {geomname: [lon, lat]}}
    nuggets : dict
        Should map the GP submodels of M to the corresponding nugget values,
        if any.
    burn : int
        Burnin iterations to discard.
    thin : int
        Number of iterations between ones that get used in the predictions.
    total : int
        Total number of iterations to use in thinning.
    fns : list of functions
        Each function should take four arguments: sofar, next, cols and i.
        Sofar may be None.
        The functions will be applied according to the reduce pattern.
    h : dict of dicts of functions.
        {postproc: geomcoll: lambda integral1, integral2, integral3, ... : return}
    g : dict of dict of functions
        {postproc: geomcoll: geomcoll: lambda fs, [lon, lat, t], variables : return.}
    pred_covariate_dict : dict of dict of dicts of arrays
        {geomcoll: geomcoll: covariate: [val]}.
    finalize : function (optional)
        This function is applied to the products before returning. It should
        take a second argument which is the actual number of realizations
        produced.
        
Output is a dictionary of form {postproc: {geomcoll: {finalization: value}}}
"""
products = []
def extract_covariates(x, covariate_names, path):
    if len(covariate_names)>0:
        raise NotImplementedError
    else:
        return {}
for i in xrange(o.reps):
    s = 'Doing repetition %i of %i'%(i, o.reps)
    print s+'\n'+'='*len(s)
    covariate_dict = {}
    # Draw points
    for name, coll in mps.iteritems():
        keys = coll.keys()
        geoms = [coll[k]['geom'] for k in keys]
        if temporal:
            tlims = [(coll[k]['tmin'], coll[k][tmax]) for k in keys]
        else:
            tlims = None
        
        points[name] = dict(zip(keys,draw_points(geoms, o.points, weights, cov_lon, cov_lat, tlims, o.coordinate_time, o.time_points)))
        # covariate_dict[name] = dict(zip(keys, extract_covariates(x, k, o.raster_path)))
        covariate_dict[name] = dict(zip(keys, [extract_covariates(points[name][k], all_covariate_keys, o.raster_path) for k in keys]))
        
    h = {}
    g = {}
    for ap in mod.areal_postproc:
        h[ap] = {}
        g[ap] = {}
        for name, coll in mps.iteritems():
            h[ap][name], g[ap][name] = ap(coll)
    
    products.append(hdf5_to_areal_samps(M, points, nuggets, o.burn, o.thin, o.total, reduce_fns + extra_reduce_fns, h, g, covariate_dict, finalize, o.continue_past_npd))

# Write out.
# Products is [{geomcoll: {postproc: {finalization: value}}}]
# Want to write out in order {geomcoll: {postproc: {finalization: [value]}}}
