# Copyright (C) 2010 Anand Patil
# 
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
# 
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
# 
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from optparse import OptionParser
import os, sys, time, imp, datetime

# Create option parser
req_doc = """

mbg-areal-predict  Copyright (C) 2010 Anand Patil
This program comes with ABSOLUTELY NO WARRANTY.
This is free software, and you are welcome to redistribute it under certain conditions.
See <http://www.gnu.org/licenses/> for the terms of the license.

See the PDF documentation of generic-mbg.

  module                The module from which maps are to be generated.
  database-file         The name of the database file produced by the mcmc.
  burn                  The number of initial MCMC iterations to discard. 
                        You must provide this argument.
  polyfile              A file containing the regions to consider in GeoJSON
                        or XML format. See the PDF documentation.
"""
p = OptionParser('usage: %prog module database-file burn mask [options]' + req_doc)
p.add_option('-r','--reps',help='The number of repetitions to do, for purposes of computing Monte Carlo standard error. Defaults to 10.',dest='reps',type='int')
p.add_option('-n','--n-bins',help='The number of bins to use when creating histograms. Defaults to 100.',dest='n_bins',type='int')
p.add_option('-x','--points',help='The number of spatial points to use for estimating areal integrals. Defaults to 100.',dest='points',type='int')
p.add_option('-i','--iter',help="The number of posterior predictive draws to take for each repetition. defaults to 1000.",dest='total',type='int')
p.add_option('-t','--thin',help='How much to thin the MCMC trace. Defaults to 10.',dest='thin',type='int')
p.add_option('-w','--weight-raster',help="An optional raster giving relative weights. This can be used if weighted samples will give a better approximation to the integral than multiplying the integrand by the weight.",dest='weight_raster')
p.add_option('-c','--coordinate-time',help="If 0, (x,t) samples are distributed randomly in the admin unit X the time interval. If 1, then to each 'x' sample there correspond multiple 't' samples and vice versa. Set this to true if your f's need to take an integral in time (or space), apply a nonlinear transformation, and then take an integral in space (or time).",dest='coordinate_time')
p.add_option('-m','--time-points',help="If the coordinate-time flag is set, this option determines how many time points are stacked up. Defaults to 10.",dest='time_points')
p.add_option('-d','--ignore-npd',help='Whether to discard and continue past non-positive-definite iterations.',dest='continue_past_npd',type='int')
p.add_option('-q','--quantiles',help="The quantile maps to generate. Should be in the form '0.05 0.25 0.5 0.75 0.95', and the inverted commas are important! Defaults to '0.05 0.25 0.5 0.75 0.95'",dest='quantile_list')
p.add_option('-p','--raster-path',help="The path to the covariate rasters. Defaults to the current working directory.",dest='raster_path')
p.add_option('-u','--quantile-uplim',help='The upper limit of the mapped quantiles',type='float',dest='quantile_uplim')
p.add_option('-l','--quantile-lolim',help='The lower limit of the mapped quantiles',type='float',dest='quantile_lolim')


p.set_defaults(reps=10)
p.set_defaults(points=100)
p.set_defaults(total=1000)
p.set_defaults(weight_raster=None)
p.set_defaults(coordinate_time=0)
p.set_defaults(continue_past_npd=0)
p.set_defaults(thin=10)
p.set_defaults(quantile_list='0.05 0.25 0.5 0.75 0.95')
p.set_defaults(raster_path=os.getcwd())
p.set_defaults(quantile_uplim=1.)
p.set_defaults(quantile_lolim=0.)
p.set_defaults(n_bins=100)
p.set_defaults(time_points=10)


# Parse command-line arguments and make imports.
o, args = p.parse_args()
from generic_mbg import *
check_args(args, 4)
if o.coordinate_time:
    raise NotImplementedError, "Haven't implemented time coordination yet."
o.module, o.hf_name, o.burn, o.polyfile_name = args
o.burn = int(o.burn)
exec(delayed_import_str)

# Restore model and tagged attributes.
M, hf, mod, mod_name = reload_model(o.module, o.hf_name)
nuggets, obs_labels = get_nuggets_and_obs(mod, mod_name, M)
extra_reduce_fns, extra_finalize = get_reduce_finalize(mod)

q=parse_quantiles(o)
temporal = ('t' in hf.root.input_csv.colnames)
bins = np.linspace(0,1,o.n_bins+1)

# Create covariate locations
ck = copy.copy(M.covariate_keys)
cov_bbox = None
try:
    ck.remove('m')
except ValueError:
    pass
if len(ck)>0:
    x, unmasked, output_type = raster_to_locs(ck[0], thin=1, bufsize=0, path=o.raster_path)
    cov_lon, cov_lat, cov_data, cov_type = import_raster(ck[0], o.raster_path)
    cov_mask = cov_data.mask
    cov_bbox = x[:,0].min(), x[:,1].min(), x[:,0].max(), x[:,1].max()
    all_covariate_keys = M.covariate_keys
    all_covariate_keys.remove('m')
else:
    all_covariate_keys = []

if o.weight_raster is not None:
    if cov_bbox is None:
        x, unmasked, output_type = raster_to_locs(o.weight_raster, thin=1, bufsize=0, path=o.raster_path)
        cov_lon, cov_lat, weights, cov_type = import_raster(o.weight_raster, o.raster_path)
        cov_mask = weights.mask
        cov_bbox = x[:,0].min(), x[:,1].min(), x[:,0].max(), x[:,1].max()   
    else:     
        # Using raster_to_vals with raw=True guarantees consistency with the covariate raster.
        weights = raster_to_vals(o.weight_raster, path=o.raster_path, thin=1, unmasked=unmasked, raw=True)
                
else:
    weights, cov_lon, cov_lat, cov_mask, cov_type, weights_in_unit, points_in_unit = None, None, None, None, None, None, None
    unmasked = None

# Create standard accumulators and finalizers for posterior summaries
if len(q)>0:
    def binfn(arr,n_bins=o.n_bins,qu=o.quantile_uplim,ql=o.quantile_lolim):
        return np.array(((arr-ql)/(qu-ql))*n_bins,dtype=int)

    hsr = histogram_reduce(bins, binfn)
    hsf = histogram_finalize(bins, q, hsr)

def finalize(prod, n, q=q, ef=extra_finalize, qu=o.quantile_uplim, ql=o.quantile_lolim):
    mean = prod[mean_reduce] / n
    var = prod[var_reduce] / n - mean**2
    std = np.sqrt(var)
    std_to_mean = std/mean
    samps = np.array(prod[sample_reduce])
    out = {'mean': mean, 'var': var, 'std': std, 'std-to-mean':std_to_mean,'samples':samps}
    if len(q)>0:
        fin = hsf(prod, n)
        out.update(dict([(k,fin[k]*(qu-ql)+ql) for k in fin.iterkeys()]))
    if ef is not None:
        out.update(ef(prod, n))
    return out

reduce_fns = [mean_reduce, var_reduce, sample_reduce]
if len(q)>0:
    reduce_fns = reduce_fns + [hsr]

polyfile_ext = os.path.splitext(o.polyfile_name)[1]
if polyfile_ext=='.json':
    mps = slurp_geojson(o.polyfile_name, temporal, cov_bbox, cov_lon, cov_lat, cov_mask)
elif polyfile_ext=='.xml':
    mps = slurp_xml(o.polyfile_name, temporal, cov_bbox, cov_lon, cov_lat, cov_mask)

print 'Clipping weight raster to units.'
t1 = time.time()
if weights is not None:
    weights_in_geom = {}
    points_in_geom = {}
    for name, coll in mps.iteritems():
        keys = coll.keys()
        geoms = [coll[k]['geom'] for k in keys]
        for geom in geoms:
            weights_in_geom[geom], points_in_geom[geom] = get_weights_in_geom(geom, weights, cov_lon, cov_lat)
else:
    weights_in_geom, points_in_geom = None, None
print 'Clipping done in %f seconds'%(time.time()-t1)

points = {}
products = []
def extract_covariates(x, covariate_names, path):
    # FIXME: implement this.
    if len(covariate_names)>0:
        raise NotImplementedError
    else:
        return {}

t_start = time.time()
for i in xrange(o.reps):
    s = 'Doing repetition %i of %i'%(i, o.reps)
    print s+'\n'+'='*len(s)
    covariate_dict = {}
    
    # Draw points and extract covariates there.
    for name, coll in mps.iteritems():
        keys = coll.keys()
        geoms = [coll[k]['geom'] for k in keys]
        if temporal:
            tlims = [(coll[k]['tmin'], coll[k]['tmax']) for k in keys]
        else:
            tlims = None
        
        points[name] = dict(zip(keys,draw_points(geoms, o.points, weights_in_geom, points_in_geom, tlims, o.coordinate_time, o.time_points)))
        covariate_dict[name] = dict(zip(keys, [extract_covariates(points[name][k], all_covariate_keys, o.raster_path) for k in keys]))
    
    # Determine the (h,g) pairs for this geometry collection.    
    h = {}
    g = {}
    for ap in mod.areal_postproc:
        h[ap] = {}
        g[ap] = {}
        for name, coll in mps.iteritems():
            h[ap][name], g[ap][name] = ap(coll)
    
    # Do one areal prediction.
    products.append(hdf5_to_areal_samps(M, points, nuggets, o.burn, o.thin, o.total, reduce_fns + extra_reduce_fns, h, g, covariate_dict, finalize, o.continue_past_npd))
print '\nAreal predictions produced in %f seconds\n'%(time.time() - t_start)

# Write out.
# Products is [{geomcoll: {postproc: {finalization: value}}}]
# Want to write out in order {geomcoll: {postproc: {finalization: [value]}}}

init_output_dir(o,'areal-predictions')
hf_out = tb.openFile('areal-predictions.hdf5','w')
desc = {}
finalizations = products[0].values()[0].values()[0]
for k,v in finalizations.iteritems():
    desc[k] = tb.Float64Col(shape=np.shape(v))
for geomcoll in products[0].iterkeys():
    g=hf_out.createGroup('/',geomcoll.replace('/','_or_'))
    for k,v in products[0][geomcoll].iteritems():
    	t=hf_out.createTable('/'+geomcoll,k.__name__,desc)
    	hf_out.flush()
    	row = t.row
    	for i in xrange(o.reps):
    	    for k_,v_ in products[i][geomcoll][k].iteritems():
    	    	row[k_]=v_
    	    row.append()
    	    hf_out.flush()

