% Copyright (C) 2010 Anand Patil
% 
% This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License.
% See view-source:http://creativecommons.org/licenses/by-nc-sa/3.0/legalcode.

\chapter{Statistician's guide} 
\label{chap:stat}

This chapter explains how to write specializing modules. A specializing module is a Python package that defines a PyMC probability model. It needs to expose certain attributes that tell \texttt{generic-mbg} how to interpret the variables the model contains.

This chapter assumes you know how to use PyMC. See the \href{http://code.google.com/p/pymc/}{PyMC user guide} if you don't.




\section{File structure} 
Try the following from the shell, on a single line:
\begin{verbatim}
    mbg-init-specializing-module duffy-vivax 
        -e 'cut_matern, utils' 
        -a 'Anand Patil, Carlos Guerra, Rosalind Howes'
\end{verbatim}
The command \texttt{mbg-init-specializing-module} is there to help you get the file structure of your specializing module right. It will create Fortran extensions according to the \texttt{-e} option, add the commands to compile them in \texttt{setup.py}, give the package's copyright to the given authors and license it under the GPL. It will also initialize a git repository in the new directory, and make the initial commit.


Take a moment to browse the directory tree you've created, which should look like this:
\begin{center}
\includegraphics[width=12cm]{filestructure.png}     
\end{center}
Pay special attention to the \texttt{\_\_init\_\_.py} file, which lists the attributes you'll need to define to tell \texttt{generic-mbg} how to use your specializing module.

\subsection{Importing}
You can install the new package as follows:
\begin{verbatim}
    cd duffy-vivax
    python setup.py develop
\end{verbatim}
See section \ref{sec:spec-local} for more detail. Now change directory to \texttt{\~}, start the Python interpreter, and import the \texttt{duffy\_vivax} module. Browse through its namespace. Pay special attention to the calling conventions of the Fortran subroutines you've generated:
\begin{verbatim}
    In [2]: duffy_vivax.utils.dummy?
    Type:           fortran
    String Form:    <fortran object at 0x1067c4828>
    Namespace:      Interactive
    Docstring:
        dummy - Function signature:
          y = dummy(x)
        Required arguments:
          x : input rank-1 array('d') with bounds (n)
        Return objects:
          y : rank-1 array('d') with bounds (n)
\end{verbatim}
The \texttt{f2py} directives in \texttt{utils.f} (the lines beginning with \texttt{cf2py}) determine which arguments to the subroutine are inputs you need to provide from Python, which inputs Python should see as outputs, and which should be inferred from other arguments. Learning how to write your own Fortran extensions is very worthwhile.

\section{Writing the model}

The primary attribute of a specializing module is a function called \texttt{make\_model}, henceforth the `model factory'. The model factory should take the following arguments:
\begin{description}
    \item[lon] An array of longitudes, in radians.
    \item[lat] An array of latitudes, in radians, of the same length as \texttt{lon}.
    \item[t (optional)] An array of times, in decimal years, of the same length as \texttt{lon}. Spatiotemporal models only.
    \item[covariate\_dict] A dictionary mapping labels to arrays. Each array should give the evaluation of a covariate surface on the \texttt{[lon,lat,t]} input array defined so far.
    \item[**non\_cov\_columns] Any number keyword arguments whose values are arrays of the same length as \texttt{lon}.
\end{description}
These arguments will be read from a user-supplied datafile and fed into the model factory by \texttt{mbg-infer}. The model factory should return its local variables as follows: 
\begin{verbatim}
    def make_model(lon,lat,covariate_dict,**non_cov_columns):
        ...
        return locals()
\end{verbatim}

\subsection{The fields}

\texttt{Generic-mbg} handles probability models containing one or more Gaussian random fields, whose means are linear combinations of covariate surface evaluations with normally-distributed coefficients, with normally-distributed nuggets:
\begin{equation}
    \label{eq:canonical} 
    \begin{array}{r}
        \beta \stackrel{\textup{\tiny iid}}{\sim} \textup{N}(0,V_{\beta,i}) \\\\
        M:x\rightarrow f_0(x;\phi) + \beta_0 + \sum_{i=1}^n \beta_i c_i(x)\\\\
        C:x,y\rightarrow K(x,y;\theta)\\\\
        f \sim \textup{GP}(M,C)\\\\
        f_x = f(x) \sim \textup{N}(M(x), C(x,x))\\\\
        g_x \sim \textup{N}(f_x, V) 
    \end{array}
\end{equation}
where $K$ is a positive definite covariance function with parameters $\theta$ and $f_0$ is any old function with parameters $\phi$. \texttt{Generic-mbg} does not care what priors you use for $\theta$, $\phi$, $V$ or the $V_\beta$'s, but the coefficients $\beta$ must be iid and normally distributed. Similarly, the package doesn't care how any other variables depend on any of the variables in the model. There may be any number of these Gaussian process submodels in a model.

The nugget and covariates are optional. Without both, the model would be simplified:
\begin{eqnarray*}
    \beta_0\sim\textup{N}(0,V_{\beta,0})\\
    M:x\rightarrow f_0(x;\phi) + \beta_0\\
    C:x,y\rightarrow K(x,y;\theta)\\
    f \sim \textup{GP}(M,C)\\
    f_x = f(x) \sim \textup{N}(M(x), C(x,x))
\end{eqnarray*}

\subsection{Integrating out the covariates}

Using standard multivariate normal transformation rules, the first four lines of model (\ref{eq:canonical}):
\begin{eqnarray*}
    \beta \stackrel{\textup{\tiny iid}}{\sim} \textup{N}(0,V_{\beta,i}) \\
    M:x\rightarrow f_0(x;\phi)+\beta_0 + \sum_{i=1}^n \beta_i c_i(x)\\
    C:x,y\rightarrow K(x,y;\theta)\\
    f \sim \textup{GP}(M,C)
\end{eqnarray*}
can be marginalized to obtain the following model:
\begin{equation}
    \label{eq:int-covariates} 
    \begin{array}{r}
    M:x\rightarrow f_0(x;\phi)\\\\
    C:x,y\rightarrow K(x,y;\theta) + V_{\beta_0} + \sum_{i=1}^n V_{\beta,i}c_i(x)c_i(y)^T\\\\
    f\sim\textup{GP}(M,C)
    \end{array}
\end{equation}
This parameterization is required in \texttt{generic-mbg}, because experience shows that the MCMC chains tend to mix much better. Section \ref{sub:example} shows how to do this easily. The shell command \texttt{mbg-covariate-traces} can be used to obtain MCMC traces for the $\beta$'s after the MCMC is complete.

\subsection{The nugget and mixing}

The last two lines of model \ref{eq:canonical},
\begin{eqnarray*}
    f_x = f(x) \sim \textup{N}(M(x), C(x,x))\\
    g_x \sim \textup{N}(f_x, V)
\end{eqnarray*}
are slightly nonstandard. Usually, the nugget variance $V$ would be incorporated in the covariance function, and $f_x$ would not be directly imputed. Keeping $f_x$ and $g_x$ separate is strongly recommended in \texttt{generic-mbg}, however. Experience has shown that the following jumping strategy:
\begin{enumerate}
    \item Metropolis sample each element of $g_x$ one at a time
    \item Gibbs sample $f_x$ jointly conditional on $g_x$
\end{enumerate}
performs well in a wide range of situations. The second step can be performed using the step method \texttt{GPEvaluationGibbs}, which is provided by \texttt{PyMC}. Section \ref{sub:mcmc-init} describes how to use \texttt{GPEvaluationGibbs}. 

\subsection{Example field}
\label{sub:example} 

Let's make the submodel in equation \ref{eq:canonical}, reparameterized as in equation \ref{eq:int-covariates}. First, the covariance function and its parameters:
\begin{verbatim}
    amp = pymc.Exponential('amp', .1, value=1.)
    scale = pymc.Exponential('scale_shift', .1, value=.08)
    diff_degree = pymc.Uniform('diff_degree', .01, 1.5)
    covariate_variances = pymc.Uniform('covariate_variances',0,1,size=n_covariates)
    x = numpy.hstack((lon,lat)).T
    
    @pymc.deterministic
    def C(amp=amp, scale=scale, diff_degree=diff_degree, 
            covariate_variances=covariate_variances):

        facdict = dict([(k,1e2*covariate_variances[i]\
            for i,k in enumerate(covariate_names))])
            
        facdict['m']=1e4

        eval_fun = generic_mbg.CovarianceWithCovariates(pm.gp.cov_funs.matern.geo_rad,
            x, covariate_values, fac=facdict)

        return pymc.gp.FullRankCovariance(eval_fun, amp=amp, scale=scale, 
            diff_degree=diff_degree)
\end{verbatim}
\texttt{Generic-mbg} provides the wrapper class \texttt{CovarianceWithCovariates}, which creates a covariance function of the form shown in (\ref{eq:int-covariates}). The key \texttt{'m'} is understood to correspond to the constant term. As formulated, the variances of the covariate coefficients are free parameters in the model, meaning the model can simplify itself. Now a trivial mean function:
\begin{verbatim}
    M = pymc.gp.Mean(lamda x: numpy.zeros(x.shape[0]))
\end{verbatim}

The PyMC factory class \texttt{GPSubmodel} incorporates a Gaussian process with the given mean and covariance into the model:
\begin{verbatim}
    S = pymc.gp.GPSubmodel('S', M, C, x)
\end{verbatim}

Now we're ready to create $g_x$:
\begin{verbatim}
    g_x = pymc.Normal('g_x', mu=S.f_eval, tau=1/V)
\end{verbatim}
PyMC parameterizes the normal distribution by precision rather than variance, so the second parameter of $g_x$ is $\tau=1/V$. The mean is the \texttt{f\_eval} attribute of the GP submodel, which gives the GP's evaluation on the input mesh, in this case \texttt{x}. That's all there is to producing GP submodels in \texttt{generic-mbg}. You can complete the model any way you like: add more GP submodels, likelihoods, etc.

\subsection{Be sure to commit}
\label{sub:git-commit} 
It's important to commit any changes you make to the specializing module to the local repository before running any MCMC, because the hash of the current git head is stored in the MCMC trace. If the state of the code doesn't correspond to the hash, you may not be able to reproduce your results later.

\section{Required attributes}

The specializing module needs to expose several attributes in addition to \texttt{make\_model}. See \texttt{\_\_init\_\_.py} in the package you just created, \texttt{duffy\_vivax}

\subsection{Metadata and non-covariate columns}
\subsubsection{Non-covariate columns} 
The attribute \texttt{non\_cov\_columns} is a dictionary mapping labels to type strings (such as \texttt{'float'}, \texttt{'int'}, \texttt{'str'}, etc.). \texttt{Generic-mbg} will expect to find the labels as column headers in any CSV datafiles provided by the end user. 
\subsubsection{Arbitrary metadata} 
The attribute \texttt{metadata\_keys} allows you to store any local variable generated by the model factory in the trace. If your \texttt{metadata\_keys} are \texttt{['n\_tacos', 'fillings']}, then the corresponding variables can be retrieved from the trace file using \texttt{hf.root.metadata.n\_tacos[0]}, etc.

\subsection{Checking the dataset}
The attribute \texttt{check\_data} is a function that takes the input dataset as a NumPy record array and performs any checks desired, raising errors if they fail. It examines the dataset before the model is created.

\subsection{Adding extra summary products}
The scripts \texttt{mbg-map} and \texttt{mbg-evaluate-survey} can map the mean, variance, standard deviation, standard deviation-to-mean ratio, quantiles, and credible intervals of specified functions (determined by \texttt{map\_postproc}) of the model's fields and other variables. Similarly, \texttt{mbg-areal-predict} produces these summaries for functions of spatial integrals. If maps of other posterior summaries are desired, the attributes \texttt{extra\_reduce\_fns} and \texttt{extra\_finalize} can be defined.

The attribute \texttt{extra\_reduce\_fns} should be a list of functions, each of which takes arguments \texttt{sofar}, \texttt{next} and \texttt{name}. These functions are accumulators. The argument \texttt{next} is a new, realized map, and the argument \texttt{sofar} is the accumulated product given the maps seen so far (which is initialized to \texttt{None}). The return value should be a new accumulated product like \texttt{sofar}.

After all posterior samples have been generated, \texttt{generic\_mbg} produces a dictionary called \texttt{products} whose keys are the elements of \texttt{extra\_reduce\_fns} and whose values are the final accumulated products. This dictionary is passed to \texttt{extra\_finalize} along with the total number of samples drawn. \texttt{extra\_finalize} should return a dictionary whose keys will be converted to filenames, and whose values should be maps for \texttt{mbg-map} and \texttt{mbg-evaluate-survey} or other values for \texttt{mbg-areal-predict}.

Currently the same reducing and finalizing functions are applied to all functions of the model variables and for all tasks. More fine-grained control may be permitted in the future.

\subsection{Tagging the nuggets of the Gaussian process submodels}
\label{sub:variable-tags} 
The following variables must be defined in a specializing module to tag certain variables created by the model factory as playing certain roles in GP submodels:
\begin{description}
    \item[\texttt{nugget\_labels}] A dictionary mapping the names of the GP submodels to the labels of their associated nugget variances, ie \texttt{\{'S': 'V'\}}.
    \item[\texttt{obs\_labels}] A dictionary mapping the names of the GP submodels to the labels of their associated evaluations with nugget, ie \texttt{\{'S': 'g\_x'\}}.
\end{description}


\subsection{Map generation}
\label{sub:map} 
The specializing module must expose a list of functions called \texttt{map\_postproc}. Each of these functions should take arguments corresponding to all the Gaussian process submodels in the model, and return... whatever it is you want to show the per-pixel predictive distribution of in the maps. For example, if your model is a spatial GLM with count data and an inverse-logit link function, and the name of the lone GP submodel in the model is $S$, this function would do:
\begin{verbatim}
    def prevalence(S):
        return generic_mbg.invlogit(S)
    map_postproc = [prevalence]
\end{verbatim}
\texttt{Generic-mbg}'s link functions should be preferred to all others when possible, because they're multithreaded.

The map postprocessing function can take additional arguments corresponding to tallied variables in the probability model. For example, if you wanted to use Stukel's link function, and had shape variables $a_1$ and $a_2$ in the model:
\begin{verbatim}
    def prevalence(S, a1, a2):
        return generic_mbg.stukel_invlogit(S, a1, a2)
    map_postproc = [prevalence]
\end{verbatim}
Note that the GP submodel arguments are understood to be per-pixel realizations on the prediction grid with nugget variance incorporated.

\subsection{Cross-validation} 
The \texttt{generic-mbg} validation scheme is to choose one or more summaries of the data that can be considered the results of random trials, and to compare the model's prediction of the outcomes of these trials given a subset of the data to the observed outcomes in the rest of the data. The specializing module must expose a list of functions called \texttt{validate\_postproc} that define these summaries. 

The validation trials may depend on some aspects of the held-out dataset, for example the ages of the survey participants. For that reason, each of the functions in \texttt{validate\_postproc} must take a single argument, which is the held-out dataset represented as a record array. Each should return three values: the `number successful', the `number of attempts', and a function. The function should have the same calling conventions as those in \texttt{map\_postproc}: it should take the GP submodels, and optionally some variables in the model, as arguments. It should return a predictive sample for the `number successful' given the values of the model variables and possibly the `number of attempts'. For example:
\begin{verbatim}
    def prevalence(data):
        obs = data.pos
        n = data.pos + data.neg
        def f(S, a1, a2, n=n):
            return pm.rbinomial(n, generic_mbg.stukel_invlogit(S, a1, a2))
        return obs, n, f

    validate_postproc=[pr]    
\end{verbatim}

The shell command \texttt{mbg-validate} will read the non-covariate columns from the held-out datasheet, pass them into \texttt{validate\_postproc}, and use the returned function to convert the Gaussian field realizations to predictions. As with \texttt{map\_postproc}, the GP submodel arguments received will be per-pixel realizations on the held-out locations with nugget variance incorporated.

\subsection{Areal prediction}  

The task performed by the shell command \texttt{mbg-areal-predict} has a more complicated specification than the tasks performed by the other shell commands. The problem tackled by \texttt{mbg-areal-predict} is samples from, and summaries of, the posterior predictive distribution of quantities of the form
\begin{equation}
    \label{eq:areal-targ} 
    h\left(\frac{1}{|a_1|_w}\int_{a_1\times \tau_1} g_1(f_1,\ldots,f_n,x,\tau)w(x)da, \ldots,\frac{1}{|a_m|_w}\int_{a_m\times\tau_m} g_m(f_1,\ldots,f_n,x,\tau)w(x)da\right)
\end{equation}
where $a_1,\ldots,a_m$ are regions and $da$ denotes integration over area; $w$ is a weighting function and $|a|_w=\int_a w(x)da$; $\tau_1\ldots\tau_m$ are time intervals; $x$ is a geographical location; $f_1,\ldots,f_n$ are the Gaussian random fields in the model; and $g_1\ldots g_m$ and $h$ are arbitrary functions. This general specification covers many `volumetric' estimates of epidemiological interest. Three concrete examples are given below.

\begin{description}
    \item[\emph{Plasmodium falciparum} clinical burden in a country.] In this case, $n=m=1$. $f_1$ is the Gaussian random field, with nugget incorporated, corresponding to parasite rate. $g_1$ transforms this value to parasite rate (with a random age-correction factor), and thence to clinical incidence, multiplies it by the population density of the pixel containing $x$, and integrates over $\tau$, being a particular year. $h$ multiplies its argument by $|a|^2$.
    \item[The total change in \emph{Plasmodium falciparum} clinical burden in a country between two consecutive years.] In this case $m=2$ with $a_1=a_2$ and $\tau_1=[y,y+1]$ and $\tau_2=[y+1,y+2]$, $y$ being the first of the two years. $f$ and $g$ are as in the previous example. $h(z_1,z_2)=(z_2-z_1)|a|$.
    \item[The difference in areal mean HbS prevalence between Lysenko endemicity classes.] In this case $m=5$ (the number of Lysenko classes) and $n=1$. $a_i$ is Lysenko class $i$ and $\tau$ is not needed, since the model is atemporal. $f_1$ is the Gaussian random field corresponding to HbS allele frequency. $g_i$ applies the inverse link function to its argument. $h$ maps vectors of length $n$ to vectors of length $n-1$: $h(z_1,\ldots,z_n)=[z_2-z_1,\ldots,z_n-z_{n-1}]$.
    
The regions $a_1,\ldots,a_m$ and time intervals $\tau_1,\ldots,\tau_m$ are defined in the argument \texttt{polyfile} of \texttt{mbg-areal-predict}. The regions are given as \href{http://geojson.org/geojson-spec.html}{GeoJSON} polygons or multipolygons, contained in geometry collections. The associated time intervals are defined by properties \texttt{tmin} and \texttt{tmax}. An example geometry collection called \texttt{gc1} containing two regions ($r_1$ and $r_2$) is given below.
\end{description}
\begin{verbatim}
    {"type": "GeometryCollection",
    "geometries":[
        {"type": "GeometryCollection",
        "geometries": [
            {"type": "MultiPolygon", 
                "coordinates": [[[[0.64, 5.85], [0.66, 5.75], [0.26, 5.76], [-0.8, 5.21], 
                            [-1.61, 5.02], [-2.06, 4.73], [-2.93, 5.1], [-3.25, 6.61], 
                            [-2.49, 8.2], [-2.69, 9.48], [-2.83, 11], [-0.15, 11.14], 
                            [0.73, 8.32], [0.53, 6.95], [1.2, 6.1], [0.69, 5.75], 
                            [0.63, 5.95], [0.51, 6.06], [0.41, 6.08], [0.37, 6.04], 
                            [0.26, 6.1], [0.21, 6.09], [0.36, 6.02], [0.42, 6.07], 
                            [0.49, 6.04], [0.61, 5.95], [0.64, 5.85]]]], 
                "properties": {"name": "late", "tmin": 2000.0, "tmax": 2007.5}
            },

            {"type": "MultiPolygon", 
                "coordinates": [[[[0.64, 5.85], [0.66, 5.75], [0.26, 5.76], [-0.8, 5.21], 
                            [-1.61, 5.02], [-2.06, 4.73], [-2.93, 5.1], [-3.25, 6.61], 
                            [-2.49, 8.2], [-2.69, 9.48], [-2.83, 11], [-0.15, 11.14], 
                            [0.73, 8.32], [0.53, 6.95], [1.2, 6.1], [0.69, 5.75], 
                            [0.63, 5.95], [0.51, 6.06], [0.41, 6.08], [0.37, 6.04], 
                            [0.26, 6.1], [0.21, 6.09], [0.36, 6.02], [0.42, 6.07], 
                            [0.49, 6.04], [0.61, 5.95], [0.64, 5.85]]]], 
                "properties": {"name": "early", "tmin": 1993.5, "tmax": 2000.0}
            }
        ],
        "properties" : {"name": "Ghana"}
        },
        {"type": "GeometryCollection",
            "geometries": [
                {"type": "MultiPolygon", 
                    "coordinates": [[[[1.4, 9.43], [1.64, 6.22], [1.2, 6.1], [0.53, 6.95], 
                                [0.73, 8.32], [-0.15, 11.14], [0.92, 11], [0.78,  10.38], 
                                [1.4, 9.43]]]], 
                    "properties": {"name": "late", "tmin": 2000.0, "tmax": 2007.5}
                },

                {"type": "MultiPolygon", 
                    "coordinates": [[[[1.4, 9.43], [1.64, 6.22], [1.2, 6.1], [0.53, 6.95], 
                                [0.73, 8.32], [-0.15, 11.14], [0.92, 11], [0.78,  10.38], 
                                [1.4, 9.43]]]], 
                    "properties": {"name": "early", "tmin": 1993.5, "tmax": 2000.0}
                }
            ],
            "properties" : {"name": "Togo"}
            }
        ]
    }
\end{verbatim}

The file may contain multiple geometry collections, each of which is independently subjected to an estimation procedure of the form given above. When the file is read, the Python representation of the geometry collection will be a nested dictionary:
\begin{verbatim}
    {'Ghana':  {'late': {'tmin': 2000.0, 'tmax': 2007.5, 'geom': ...}, 
                'early': {'tmin': 1993.5, 'tmax': 2000.0, 'geom': ...}},
    {'Togo':   {'late': {'tmin': 2000.0, 'tmax': 2007.5, 'geom': ...}, 
                'early': {'tmin': 1993.5, 'tmax': 2000.0, 'geom': ...}}}
\end{verbatim}
where the \texttt{'geom'} entries are Shapely multipolygons.

The specializing module must expose an attribute called \texttt{areal\_postproc}, which like \texttt{map\_postproc} and \texttt{validate\_postproc} must be a list of functions. Each must take a dictionary like \texttt{'Ghana'} or \texttt{'Togo'} above as its lone argument and return two values: $h$, which must take keyword arguments corresponding to the name of each geometry in the dictionary (\texttt{'early'} and \texttt{'late'}, in these cases), and optionally any other variables in the model; and $g$, which must be a dictionary of functions corresponding to each geometry in the dictionary. Each value in $g$ must take as arguments the random fields in the model (labelled by the names of the GP submodels to which they belong), an array of locations (lon/lat or lon/lat/t for temporal models), and optionally any other variables in the model. The following specifications might implement concrete examples above:
\begin{verbatim}
    def total_prevalence(gc):
        "Total number of P. falciparum infections in a country"
        exec("h = lambda %s=%s, a=gc['%s']['geom'].area: %s*a"%((gc.keys()[0],)*4))
        def g(S,x,a1,a2,pop_dens):
            return generic_mbg.stukel_invlogit(S(x), a1, a2) * pop_dens(x)
        return h, {gc.keys()[0]: g}
    
    def diff_prevalence(gc):
        "Difference in total number of P. falciparum infections between two time periods"
        def h(early, late, a=gc['late']['geom'].area):
            return (late-early)*a
        def g(S,x,a1,a2,pop_dens):
            return generic_mbg.stukel_invlogit(S(x), a1, a2) * pop_dens(x)
        return h, {'early': g, 'late': g}
    
    def hbs_diff(gc):
        "Difference in areal mean HbS prevalence between Lysenko endemicity classes"
        def h(no, hypo, meso, hyper, holo):
            return np.diff(np.hstack((no, hypo, meso, hyper, holo)))
        g = dict([k, lambda S, x: generic_mbg.invlogit(S(x))]\
            for k,v in gc.iteritems())
        return h, g
\end{verbatim}
The \texttt{g} functions receive actual Gaussian random fields for the arguments labeled as spatial submodels, and must evaluate them appropriately using x.

\subsubsection{Temporal alignment}
By default, the $g$ function is interpreted as $\tilde g$ in $g(f_1,\ldots,f_n,x,\tau)=\frac{1}{|\tau|}\int_\tau \tilde g(f_1(x,t),\ldots,f_n(x,t))dt$. In this default case, the $x$ and $t$ arguments seen by $g$ are such that \texttt{np.hstack((x,t))} gives an array of space-time samples distributed within $a\times\tau$ either uniformly or according to the weighting raster. The two temporal examples given above fall into this case.

However, consider the case $g(f_1,\ldots,f_n,x,\tau)=k(\int_\tau\tilde g(f_1(x,t),\ldots,f_n(x,t))dt)$, where $k$ is a nonlinear transformation. The estimate \ref{eq:areal-targ} becomes difficult to compute based on uniformly-distributed $x,t$ points in $a\times \tau$. In this case, the \texttt{-c} option to \texttt{mbg-areal-predict} should be set, and $x$ will be formed from $x\times t$ where $x$ is a set of spatial locations and $t$ is a set of times. The functions $g$ should generate vectors of time points at each location in $x$ as necessary. If independent simulations are desired at different spatial locations, the spatial submodel arguments should be copied before calling.

\subsubsection{Estimation and Monte Carlo standard error}
\texttt{Mbg-areal-predict} is unique amongst the \texttt{generic-mbg} commands in that its Monte Carlo error will generally be of more concern. For this reason, its \texttt{-r} argument determines the number of replications. All computations are repeated this many times. The averages of the results are presented as estimates, and the standard deviations are scaled and presented as estimates of Monte Carlo standard error.

\textbf{Document output format once you produce a data structure}
\textbf{Note if h nonlinear, trial mean is not unbiased.}


\subsection{Survey evaluation}
\label{sub:survey-eval} 
Suppose inference on a dataset $d$ is complete, resulting in a posterior $p(\phi|d)$. Write the current value of posterior summary $m$ at location $x$ as $m(x|d)$. Given a sufficiently complete plan $s$ for a new data collection effort (the locations, times and sizes of all community samples that will be taken, say), it is possible to compute a predictive distribution for the new data $d^s$ that will be gathered, $p(d_s|d)$. The updated posterior $p(\phi|d,d_s)$ is a function of $d_s$, as is $m(x|d,d_s)$. That means that a predictive distribution $p(m(x|d,d_s)|d)$ can be derived for $m(x|d,d_s)$.
Another summary $m_2$ can be applied to this predictive distribution, resulting in $m_2(m(x|d,d_s)|d)$. Gridded maps of such summaries give an idea of how much will be learned from the survey. For example, if $m$ is the median and $m_2$ is the interquartile range, the map gives an idea of how much the median map is likely to move. If $m$ is the interquartile range and $m_2$ is the median, the map gives an idea of how much the interquartile range is likely to shrink or grow. 

Such maps are produced by \texttt{mbg-evaluate-survey}. This script requires two attributes to operate. The first is a function called \texttt{simdata\_postproc}, which should have arguments with labels corresponding to all the Gaussian process submodels in the model; \texttt{survey\_plan}; and optionally any variables in the model. The values received for the GP submodel arguments will be evaluations of the corresponding random fields, with nuggets, at the locations specified in \texttt{survey\_plan}. The latter is a NumPy record array with columns corresponding to the aspects of new surveys that can be planned ahead of time. The return value should be corresponding values for the `unplannable' survey results, such as the number of individuals who test positive in each community sample.

The second is a function called \texttt{survey\_likelihood}, which should have arguments labelled after the GP submodels; \texttt{survey\_plan}; \texttt{data}; \texttt{i}; and optionally any other variables in the model. The \texttt{data} argument should be a result of \texttt{simdata\_postproc}, and the \texttt{i} argument should be the index of a row in the survey plan. The function should return the log-likelihood of every element in the GP submodels input, in an array. All other arguments are as for \texttt{simdata\_postproc}.

\subsubsection{Interpreting the importance resampling diagnostics}
\label{subsub:importance} 
%FIXME: write this.

\subsection{Incremental data updates}
If a new dataset $d_s$ is actually collected, the script \texttt{mbg-update-dataset} can approximately incorporate it into an existing tracefile using the \texttt{survey\_likelihood} attribute, producing a new tracefile. The importance resampling diagnostics described in section \ref{subsub:importance} can help assess whether this approximate procedure has been successful. 

\subsection{Initializing the MCMC object}
\label{sub:mcmc-init} 

The shell command \texttt{mbg-infer} will call \texttt{make\_model} and wrap the jumble of variables it returns in a PyMC \texttt{MCMC} object. Before it begins sampling, it will call \texttt{mcmc\_init} on the MCMC object. This is your chance to assign custom step methods, etc. For example, to use \texttt{PyMC}'s \texttt{GPEvaluationGibbs} on the Gaussian field evaluation in section \ref{sub:example}:
\begin{verbatim}
    def mcmc_init(M):
        M.use_step_method(pymc.GPEvaluationGibbs, M.S, M.V, M.g_x)
\end{verbatim}



