# Copyright (C) 2010 Anand Patil
# 
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
# 
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
# 
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from optparse import OptionParser
import os, sys, time, imp, datetime

# Create option parser
req_doc = """

mbg-map  Copyright (C) 2009 Anand Patil
This program comes with ABSOLUTELY NO WARRANTY.
This is free software, and you are welcome to redistribute it under certain conditions.
See <http://www.gnu.org/licenses/> for the terms of the license.


  module                The module from which maps are to be generated.
  database-file         The name of the database file produced by the mcmc.
  burn                  The number of initial MCMC iterations to discard. 
                        You must provide this argument.
  survey                A CSV file containing the planned survey. Should have 
                        the same columns as the original input datafile that
                        can be measured or planned in advance of the survey.
  mask                  A raster file with some pixels missing. Maps will 
                        be generated in raster files with identical masks.
"""
p = OptionParser('usage: %prog module database-file burn mask [options]' + req_doc)
p.add_option('-n','--n-bins',help='The number of bins to use when creating histograms. Defaults to 100.',dest='n_bins',type='int')
p.add_option('-b','--bufsize',help='The size of the buffer to use, in pixels. Use 0 if raster-thin=1. Defaults to 0.',dest='bufsize',type='int')
p.add_option('-c','--credible-intervals',help="The centered credible interval posteriors to generate. Should be in the form '0.5 0.9', and the inverted commas are important! Defaults to '0.5 0.9'",dest='credible_intervals')
p.add_option('-q','--quantiles',help="The quantile maps to generate. Should be in the form '0.05 0.25 0.5 0.75 0.95', and the inverted commas are important! Defaults to '0.05 0.25 0.5 0.75 0.95'",dest='quantile_list')
p.add_option('-r','--raster-thin',help='The raster will be kriged at this level of degradation, then regridded. Set to 1 for final maps, ~10 for mild curiosity. Defaults to 1.',dest='raster_thin',type='int')
p.add_option('-t','--thin',help='How much to thin the MCMC trace. Defaults to 10.',dest='thin',type='int')
p.add_option('-i','--iter',help='The total number of samples to use in generating the map. Defaults to 20000',dest='total',type='int')
p.add_option('-p','--raster-path',help="The path to the covariate rasters. Defaults to the current working directory.",dest='raster_path')
p.add_option('-y','--year',help='The decimal year at which the map should be produced. Required for space-time models.',dest='year',type='float')
p.add_option('-d','--ignore-npd',help='Whether to discard and continue past non-positive-definite iterations.',dest='continue_past_npd',type='int')
p.add_option('-m','--match_moments',help='Whether to approximate the likelihood by matching posterior moments rather than minimizing KL divergence between true and approximate posterior. Defaults t 0.',dest='match_moments',type='int')

p.set_defaults(n_bins=100)
p.set_defaults(raster_path='.')
p.set_defaults(raster_thin=1)
p.set_defaults(thin=50)
p.set_defaults(total=1000)
p.set_defaults(bufsize=0)
p.set_defaults(year=None)
p.set_defaults(continue_past_npd=0)
p.set_defaults(credible_intervals='.5 .9')
p.set_defaults(quantile_list='.05 .25 .5 .75 .95')
p.set_defaults(match_moments=0)

o, args = parse_and_check_args(p,5)

o.module, o.hf_name, o.burn, o.survey, o.mask_name = args
o.burn = int(o.burn)

from generic_mbg import *
exec(delayed_import_str)

import decimal


# Load up given module and load its relevant contents
survey_plan = pl.csv2rec(o.survey)
M, hf, mod, mod_name = reload_model(o.module, o.hf_name)

nuggets, obs_labels = get_nuggets_and_obs(mod, mod_name, M)

if hasattr(mod, 'extra_reduce_fns'):
    extra_reduce_fns = mod.extra_reduce_fns
    extra_finalize = mod.extra_finalize
else:
    extra_reduce_fns = []
    extra_finalize = None


# Parse quantiles and CI's.
bins = np.linspace(0,1,o.n_bins)
    
if len(o.quantile_list) == 0:
    q = []
else:
    q = map(decimal.Decimal, o.quantile_list.split(' '))
    
if len(o.credible_intervals) == 0:
    pass
else:
    ci = map(decimal.Decimal, o.credible_intervals.split(' '))
    half = decimal.Decimal(1)/decimal.Decimal(2)
    q = np.sort(list(set(q) | set(np.hstack([(half*(1+cii), half*(1-cii)) for cii in ci]))))
    

# Create predictive locations
x, unmasked, output_type = raster_to_locs(o.mask_name, thin=o.raster_thin, bufsize=o.bufsize, path=o.raster_path)
if not o.year is None:
    x = np.vstack((x.T,o.year*np.ones(x.shape[0]))).T


# Load covariates
all_covariate_keys = M.covariate_keys
covariate_dict = {}
for k in all_covariate_keys:
    if k != 'm':
        try:
            covariate_dict[k] = raster_to_vals(k, path=o.raster_path, thin=o.raster_thin, unmasked=unmasked)
        except IOError:
            raise IOError, 'Covariate raster %s not found in path %s.'%(k+'.asc',o.raster_path)


# Generate simulated datasets
postproc = [close(mod.simdata_postproc,survey_plan=survey_plan)]
survey_x = np.vstack((survey_plan.lon, survey_plan.lat))*np.pi/180.
if 't' in survey_plan.dtype.names:
    survey_x = np.vstack((survey_x, survey_plan.t))
survey_x = survey_x.T
survey_covariate_dict = dict([(k,survey_plan[k]) for k in all_covariate_keys])
print 'Simulating datasets at survey locations.'
t1 = time.time()
products = hdf5_to_samps(M,survey_x,nuggets,o.burn,o.thin,o.total, [sample_reduce], postproc, survey_covariate_dict, sample_finalize, o.continue_past_npd, joint=True)
print 'Simulated datasets generated in %s seconds.'%(time.time()-t1)
samples = products[postproc[0]]['samples']


# Make output dir and temporary hdf5 file to hold intermediate results
hf_path, hf_basename  = os.path.split(o.hf_name)
base_outname = os.path.splitext(hf_basename)[0]
eval_dir = os.path.join(hf_path, base_outname+'-survey-eval')
try:
    os.mkdir(eval_dir)
except OSError:
    pass
workspace_file = tb.openFile(os.path.join(eval_dir,eval_dir+'.hdf5'),'w')

workspace_file.createArray('/','unmasked',unmasked)
        
# Prepare reducing functions
reduce_fns = [mean_reduce_with_hdf(workspace_file, len(samples)+1), var_reduce_with_hdf(workspace_file, len(samples)+1)]
n_arrays = 2

if len(q)>0 or len(ci)>0:
    n_arrays += o.n_bins
    def binfn(arr,n_bins=o.n_bins):
        return np.array(arr*n_bins,dtype=int)
    hsr = histogram_reduce_with_hdf(bins, binfn, workspace_file, len(samples)+1)
    hsf = histogram_finalize(bins, q, hsr)
    reduce_fns = reduce_fns + [hsr]

upper_gb = samples.shape[0]*x.shape[0]*n_arrays*4*len(mod.map_postproc)/1.e9
if upper_gb > 1:
    print '\n\nUpper-bound estimate of size of intermediate results file %s.hdf5 is %f gigabytes. You can decrease this by mapping on a smaller region, by setting the -r / --raster-thin argument to a larger value or by decreasing the number of things you want to map (map_postproc functions).\n\n'%(eval_dir, upper_gb)


# Do the predictive simulation
print 'Updating the posterior for each simulated dataset, and simulating on predictive raster.'
t1 = time.time()
actual_total, log_imp_weights = hdf5_to_survey_eval(M, x, nuggets, o.burn, o.thin, o.total, reduce_fns, mod.map_postproc, covariate_dict, survey_x, samples, survey_covariate_dict, mod.survey_likelihood, survey_plan, o.match_moments, finalize=None, continue_past_npd=False)
print 'Posterior predictive simulations done in %s seconds'%(time.time()-t1)


# Finalize the reduced products for each of the simulated datasets
print 'Finalizing.'
t1 = time.time()
for mp in mod.map_postproc:
    
    mean_arr = getattr(workspace_file.root, mp.__name__+'_mean')
    var_arr = getattr(workspace_file.root, mp.__name__+'_var')
        
    std_arr = workspace_file.createCArray('/',mp.__name__ + '_std', shape=mean_arr.shape,atom=tb.FloatAtom(),filters=tb.Filters(complevel=1,complib='zlib'))
    std_to_mean_arr = workspace_file.createCArray('/',mp.__name__ + '_std_to_mean', shape=mean_arr.shape,atom=tb.FloatAtom(),filters=tb.Filters(complevel=1,complib='zlib'))
    
    if len(q)>0:
        hist_arr = getattr(workspace_file.root, mp.__name__+'_histogram')
        quantile_arrs = dict([(qi, workspace_file.createCArray('/',(mp.__name__ + '_quantile_%s'%qi).replace('.','__'), shape=mean_arr.shape,atom=tb.FloatAtom(),filters=tb.Filters(complevel=1,complib='zlib'))) for qi in q])
    
    if len(ci)>0:
        ci_arrs = dict([(cii, workspace_file.createCArray('/',(mp.__name__ + '_ci_%s'%cii).replace('.','__'), shape=mean_arr.shape,atom=tb.FloatAtom(),filters=tb.Filters(complevel=1,complib='zlib'))) for cii in ci])
    
    for i in xrange(mean_arr.shape[0]):
        mean_arr[i] = mean_arr[i]/actual_total
        var_arr[i] = var_arr[i]/actual_total - mean_arr[i]**2
        std_arr[i] = np.sqrt(var_arr[i])
        std_to_mean_arr[i] = std_arr[i] / mean_arr[i]
        
        if len(q)>0:
            hist_info = hsf({hsr: hist_arr[i,:]}, actual_total)
            for qi in q:
                quantile_arrs[qi][i,:] = hist_info['quantile-%s'%qi]

        if len(ci)>0:
            for cii in ci:
                ci_arrs[cii][i,:] = hist_info['quantile-%s'%(half*(1+cii))]-hist_info['quantile-%s'%(half*(1-cii))]
print 'Finalizing done in %s seconds'%(time.time()-t1)

map_keys = ['mean','var','std','std_to_mean']+[('quantile_%s'%qi).replace('.','__') for qi in q]+[('ci_%s'%cii).replace('.','__') for cii in ci]

os.chdir(eval_dir)

# Visualize pickiness of simulated data
exp_number_retained = np.zeros(len(samples))
mass_not_in_mode = np.zeros(len(samples))
for l in xrange(len(samples)):
    imp_weights = np.exp(log_imp_weights[:,l])
    exp_number_retained[l] = np.sum(1.-(1-imp_weights)**actual_total)
    mass_not_in_mode[l] = 1-imp_weights.max()

pl.clf()
pl.hist(exp_number_retained,50)
pl.title('Total samples = %i'%actual_total)
pl.savefig('exp_number_retained.pdf')

pl.clf()
pl.hist(np.log(mass_not_in_mode),50)
pl.title('Number of posterior samples = %i'%imp_weights.shape[0])
pl.savefig('log_mass_not_in_mode.pdf')

pl.clf()
pl.hist(mass_not_in_mode,50)
pl.title('Number of posterior samples = %i'%imp_weights.shape[0])
pl.savefig('mass_not_in_mode.pdf')


def make_pdf(v, out_name):
    pdf_name = out_name + '.pdf'
    print 'Generating output file %s'%(out_name.replace('__','.'))

    lon,lat,data=vec_to_raster(v,o.mask_name,o.raster_path,out_name,unmasked)
    pl.clf()
    pl.imshow(grid_convert(data,'y+x+','y-x+'), extent=[lon.min(), lon.max(), lat.min(), lat.max()], interpolation='nearest')
    pl.colorbar()
    pl.savefig(pdf_name)

map_reduce_fns = [mean_reduce, var_reduce]
if len(q)>0:
    hsr = histogram_reduce(bins, binfn)
    hsf = histogram_finalize(bins,q,hsr)
    map_reduce_fns.append(hsr)

def map_finalize(prod, n, q=q):
    mean = prod[mean_reduce] / n
    var = prod[var_reduce] / n - mean**2
    std = np.sqrt(var)
    std[np.where(var<0)]=0    
    std_to_mean = std/mean
    std_to_mean[np.where(mean==0)]=np.inf
    std_to_mean[np.where(std==0)]=0
    out = {'mean': mean, 'var': var, 'std': std, 'std-to-mean':std_to_mean}
    if len(q)>0:
        out.update(hsf(prod, n))
    return out

for f in mod.map_postproc:
    for k in map_keys:
        hfnode = getattr(workspace_file.root, mp.__name__+'_'+k)
        v = hfnode[-1]
        make_pdf(v, f.__name__ + '_cur_' + k)
        
        products = dict([(rf, None) for rf in map_reduce_fns])
        for i in xrange(hfnode.shape[0]-1):
            for rf in map_reduce_fns:
                products[rf] = rf(products[rf], hfnode[i], rf.__name__)
                
        mapmaps = map_finalize(products, hfnode.shape[0]-1)
        for mk,v in mapmaps.iteritems():
            make_pdf(v, f.__name__+'_'+mk+'_of_'+k)